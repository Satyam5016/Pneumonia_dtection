{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da199838-85c8-42ad-ac30-8e6298fad39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5639 images belonging to 2 classes.\n",
      "Found 1121 images belonging to 2 classes.\n",
      "Found 1198 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 - 26s - 569ms/step - auc: 0.5111 - loss: 0.5876 - val_auc: 0.8369 - val_loss: 0.5434\n",
      "Epoch 2/10\n",
      "45/45 - 0s - 390us/step - auc: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 - 23s - 522ms/step - auc: 0.8455 - loss: 0.4371 - val_auc: 0.9454 - val_loss: 0.3649\n",
      "Epoch 4/10\n",
      "45/45 - 0s - 152us/step - auc: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "45/45 - 23s - 519ms/step - auc: 0.9229 - loss: 0.3069 - val_auc: 0.9510 - val_loss: 0.3710\n",
      "Epoch 6/10\n",
      "45/45 - 0s - 172us/step - auc: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "45/45 - 23s - 521ms/step - auc: 0.9390 - loss: 0.2755 - val_auc: 0.9620 - val_loss: 0.2362\n",
      "Epoch 8/10\n",
      "45/45 - 0s - 169us/step - auc: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "45/45 - 24s - 526ms/step - auc: 0.9308 - loss: 0.2917 - val_auc: 0.9629 - val_loss: 0.2604\n",
      "Epoch 10/10\n",
      "45/45 - 0s - 232us/step - auc: 0.0000e+00 - loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D,Flatten,Dense,Dropout,BatchNormalization,InputLayer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.metrics import AUC\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Set a seed value\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# 5. For layers that introduce randomness like dropout, make sure to set seed values \n",
    "#model.add(Dropout(0.25, seed=seed_value))\n",
    "\n",
    "#6 Configure a new global `tensorflow` session\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "## Set file paths to image files\n",
    "project_path = \"/Users/satyamyadav/Desktop/pneumonia\"\n",
    "train_path = project_path + \"/dataset/train/\"\n",
    "val_path = project_path + \"/dataset/val/\"\n",
    "test_path = project_path + \"/dataset/test/\"\n",
    "\n",
    "## Set up hyperparameters that will be used later\n",
    "hyper_dimension = 64\n",
    "hyper_batch_size = 128\n",
    "hyper_epochs = 100\n",
    "hyper_channels = 1\n",
    "hyper_mode = 'grayscale'\n",
    "\n",
    "## Generate batches of image data (train, validation, and test) with data augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0, \n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2, \n",
    "                                   horizontal_flip = True)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0) \n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0) \n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory = train_path, \n",
    "                                                    target_size = (hyper_dimension, hyper_dimension),\n",
    "                                                    batch_size = hyper_batch_size, \n",
    "                                                    color_mode = hyper_mode,\n",
    "                                                    class_mode = 'binary', \n",
    "                                                    seed = 42)\n",
    "val_generator = val_datagen.flow_from_directory(directory = val_path, \n",
    "                                                 target_size = (hyper_dimension, hyper_dimension),\n",
    "                                                 batch_size = hyper_batch_size, \n",
    "                                                 class_mode = 'binary',\n",
    "                                                 color_mode = hyper_mode,\n",
    "                                                 shuffle=False,\n",
    "                                                 seed = 42)\n",
    "test_generator = test_datagen.flow_from_directory(directory = test_path, \n",
    "                                                 target_size = (hyper_dimension, hyper_dimension),\n",
    "                                                 batch_size = hyper_batch_size, \n",
    "                                                 class_mode = 'binary',\n",
    "                                                 color_mode = hyper_mode,\n",
    "                                                 shuffle=False,\n",
    "                                                 seed = 42)\n",
    "\n",
    "test_generator.reset()\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(InputLayer(input_shape=(hyper_dimension, hyper_dimension, hyper_channels)))\n",
    "\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(activation='relu', units=128))\n",
    "cnn.add(Dense(activation='sigmoid', units=1))\n",
    "\n",
    "cnn.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=[AUC()])\n",
    "cnn_model = cnn.fit(train_generator, \n",
    "                              steps_per_epoch = len(train_generator), \n",
    "                              epochs = 10, \n",
    "                              validation_data = val_generator,\n",
    "                              validation_steps = len(val_generator), \n",
    "                              verbose=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff2c1fc-ffb2-43a7-86cf-2821be6c7f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Prediction: Yes, pneumonia\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def predict_pneumonia(img_path, model):\n",
    "    # Load the image\n",
    "    img = image.load_img(img_path, target_size=(hyper_dimension, hyper_dimension), color_mode=hyper_mode)\n",
    "    \n",
    "    # Convert the image to an array and preprocess it\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0  # Rescale like in training\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    # Interpret the result\n",
    "    if prediction[0] > 0.5:\n",
    "        result = \"Yes, pneumonia\"\n",
    "    else:\n",
    "        result = \"No, normal\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usage example:\n",
    "img_path = 'yes.jpeg'  # Update with your X-ray image path\n",
    "result = predict_pneumonia(img_path, cnn)\n",
    "print(\"Prediction:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8c34e-e648-4c57-b860-9c8fea73d077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
